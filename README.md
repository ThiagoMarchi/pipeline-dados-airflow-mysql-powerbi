# Projeto de Pipeline de Dados com Airflow, Docker e MySQL

## üìÑ Resumo
Este projeto demonstra a constru√ß√£o de um pipeline de dados completo (ELT - Extract, Load, Transform) utilizando ferramentas modernas de engenharia de dados. O pipeline extrai dados de ve√≠culos de uma API p√∫blica, os processa, armazena em um banco de dados MySQL e, finalmente, gera um conjunto de dados modelado e pronto para an√°lise em ferramentas de Business Intelligence como o Power BI.

O projeto foi totalmente containerizado com Docker e orquestrado com Apache Airflow.

---

## üèóÔ∏è Arquitetura do Pipeline

O fluxo de dados segue a seguinte arquitetura:

```
[Fontes de Dados] ---> [Extra√ß√£o & Carga (ELT)] ---> [Armazenamento] ---> [Visualiza√ß√£o]
      |                           |                        |                    |
[APIs P√∫blicas] ---> [Airflow (Python/Pandas)] ---> [MySQL (Docker)] ---> [Power BI]
```

---

## üõ†Ô∏è Tecnologias Utilizadas

* **Orquestra√ß√£o:** Apache Airflow
* **Containeriza√ß√£o:** Docker & Docker Compose
* **Banco de Dados:** MySQL 8.0
* **Linguagem Principal:** Python 3.12
* **Bibliotecas Python:** Pandas, SQLAlchemy, Requests, Faker
* **Ferramenta de BI:** Microsoft Power BI
* **Ambiente de Desenvolvimento:** WSL 2 (Ubuntu)

---

## üìÅ Estrutura do Projeto

```
/meu_novo_pipeline
|
‚îú‚îÄ‚îÄ dags/                  # Cont√©m os arquivos .py das DAGs do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ dag_build_dim_models.py
‚îÇ   ‚îú‚îÄ‚îÄ dag_build_dim_dealers_fictitious.py
‚îÇ   ‚îî‚îÄ‚îÄ dag_build_fact_sales.py
|
‚îú‚îÄ‚îÄ logs/                  # Logs gerados pelo Airflow (ignorado pelo .gitignore)
|
‚îú‚îÄ‚îÄ plugins/               # Para plugins customizados do Airflow (vazio neste projeto)
|
‚îú‚îÄ‚îÄ scripts/               # Scripts auxiliares, como o de popular a dim_date
‚îÇ   ‚îî‚îÄ‚îÄ populate_dim_date.py
|
‚îú‚îÄ‚îÄ .gitignore             # Arquivo que especifica o que o Git deve ignorar
‚îú‚îÄ‚îÄ docker-compose.yaml    # Arquivo principal que define e orquestra todos os servi√ßos
‚îú‚îÄ‚îÄ Dockerfile             # Define a imagem customizada do Airflow com depend√™ncias extras
‚îî‚îÄ‚îÄ requirements.txt       # Lista de depend√™ncias Python a serem instaladas na imagem Docker
```

---

## üöÄ Como Executar o Projeto

Siga os passos abaixo para recriar e executar este ambiente.

### Pr√©-requisitos
* [Git](https://git-scm.com/)
* [Docker Desktop](https://www.docker.com/products/docker-desktop/)
* [WSL 2](https://learn.microsoft.com/pt-br/windows/wsl/install) instalado e configurado no Windows.

### Passos de Instala√ß√£o

1.  **Clonar o Reposit√≥rio**
    ```bash
    git clone [https://github.com/seu-usuario/pipeline-dados-airflow-mysql-powerbi.git](https://github.com/seu-usuario/pipeline-dados-airflow-mysql-powerbi.git)
    cd pipeline-dados-airflow-mysql-powerbi
    ```

2.  **Ajustar Permiss√µes de Pastas**
    O Airflow no Docker precisa de permiss√µes de escrita nas pastas de logs e plugins.
    ```bash
    sudo mkdir -p ./logs ./plugins
    sudo chown -R 50000:0 ./logs ./plugins
    ```

3.  **Construir e Iniciar os Cont√™ineres**
    Este comando ir√° construir a imagem customizada do Airflow (com `Faker` instalado) e iniciar todos os servi√ßos (Airflow, MySQL, Redis).
    ```bash
    docker-compose up -d --build
    ```
    Aguarde alguns minutos para que todos os servi√ßos estejam no ar.

4.  **Configurar a Conex√£o no Airflow**
    * Acesse a interface do Airflow em `http://localhost:8080` (login: `admin`, senha: `admin`).
    * V√° em **Admin -> Connections** e crie a conex√£o para o banco de dados da aplica√ß√£o:
        * **Connection Id:** `mysql_default`
        * **Connection Type:** `MySQL`
        * **Host:** `mysql_db`
        * **Schema:** `dados_api`
        * **Login:** `mysql_user`
        * **Password:** `mysql_pass`
        * **Port:** `3306`
    * Teste e salve a conex√£o.

5.  **Popular a Tabela de Datas**
    O pipeline depende de uma tabela de calend√°rio (`dim_date`). Execute o script Python para popul√°-la uma √∫nica vez.
    ```bash
    # Instale as depend√™ncias no seu ambiente WSL primeiro
    python3 -m pip install pandas sqlalchemy mysql-connector-python

    # Execute o script
    python3 scripts/populate_dim_date.py
    ```
    
6.  **Executar as DAGs**
    * Na interface do Airflow, ative e execute as DAGs na seguinte ordem:
        1.  `build_dim_dealers_fictitious`
        2.  `build_dim_models`
        3.  `build_fact_sales` (ela esperar√° as outras duas terminarem)

---

## üìä An√°lises no Power BI

Com o Data Mart populado, conecte o Power BI ao banco de dados `dados_api` (Host: `localhost`, Porta: `3306`, Usu√°rio: `mysql_user`) para criar um dashboard interativo e responder perguntas como:
* Qual o faturamento total por marca e segmento?
* Como as vendas de ve√≠culos el√©tricos evolu√≠ram ao longo dos anos?
* Qual o pre√ßo m√©dio de venda para carros de luxo vs. carros de entrada?

---

## üë®‚Äçüíª Autor

**[Seu Nome]**

* [LinkedIn](URL_DO_SEU_LINKEDIN)
* [GitHub](URL_DO_SEU_GITHUB)
