# Projeto de Pipeline de Dados com Airflow, Docker e MySQL

## üìÑ Resumo

Este projeto demonstra a constru√ß√£o de um pipeline de dados completo (ELT - Extract, Load, Transform) utilizando ferramentas modernas de engenharia de dados. O pipeline extrai dados de ve√≠culos de uma API p√∫blica, os processa, armazena em um banco de dados MySQL e, finalmente, gera um conjunto de dados modelado e pronto para an√°lise em ferramentas de Business Intelligence como o Power BI.

O projeto foi totalmente containerizado com Docker e orquestrado com Apache Airflow, seguindo uma arquitetura modular com DAGs especializadas para cada etapa do processo.

---

## üìÑ Dashboard Final

O resultado final do pipeline √© um dashboard interativo no Power BI que permite a an√°lise de vendas de ve√≠culos por diversas dimens√µes.

![Dashboard Screenshot](https://github.com/ThiagoMarchi/pipeline_airflow_mysql_powerbi/blob/main/dashboard.png)

---

## üèóÔ∏è Arquitetura do Pipeline

O fluxo de dados segue a seguinte arquitetura:

```
[Fontes de Dados] ---> [Extra√ß√£o & Carga (ELT)] ---> [Armazenamento] ---> [Visualiza√ß√£o]
      |                           |                        |                    |
[APIs P√∫blicas] ---> [Airflow (Python/Pandas)] ---> [MySQL (Docker)] ---> [Power BI]
```

---

## üõ†Ô∏è Tecnologias Utilizadas

* **Orquestra√ß√£o:** Apache Airflow
* **Containeriza√ß√£o:** Docker & Docker Compose
* **Banco de Dados:** MySQL 8.0
* **Linguagem Principal:** Python 3.12
* **Bibliotecas Python:** Pandas, SQLAlchemy, Requests, Faker
* **Ferramenta de BI:** Microsoft Power BI
* **Ambiente de Desenvolvimento:** WSL 2 (Ubuntu)

---

## üìÅ Estrutura do Projeto

```
/meu_novo_pipeline
|
‚îú‚îÄ‚îÄ dags/                  # Cont√©m os arquivos .py das DAGs do Airflow
‚îÇ   ‚îú‚îÄ‚îÄ dag_build_dim_date.py
‚îÇ   ‚îú‚îÄ‚îÄ dag_build_dim_dealers_fictitious.py
‚îÇ   ‚îú‚îÄ‚îÄ dag_build_dim_models.py
‚îÇ   ‚îî‚îÄ‚îÄ dag_build_fact_sales.py
|
‚îú‚îÄ‚îÄ logs/                  # Logs gerados pelo Airflow (ignorado pelo .gitignore)
|
‚îú‚îÄ‚îÄ plugins/               # Para plugins customizados do Airflow (vazio neste projeto)
|
‚îú‚îÄ‚îÄ .gitignore             # Arquivo que especifica o que o Git deve ignorar
‚îú‚îÄ‚îÄ docker-compose.yaml    # Arquivo principal que define e orquestra todos os servi√ßos
‚îú‚îÄ‚îÄ Dockerfile             # Define a imagem customizada do Airflow com depend√™ncias extras
‚îî‚îÄ‚îÄ requirements.txt       # Lista de depend√™ncias Python a serem instaladas na imagem Docker
```

---

## üöÄ Como Executar o Projeto

Siga os passos abaixo para recriar e executar este ambiente.

### Pr√©-requisitos
* [Git](https://git-scm.com/)
* [Docker Desktop](https://www.docker.com/products/docker-desktop/)
* [WSL 2](https://learn.microsoft.com/pt-br/windows/wsl/install) instalado e configurado no Windows.

### Passos de Instala√ß√£o

1.  **Clonar o Reposit√≥rio**
    ```bash
    git clone https://github.com/ThiagoMarchi/pipeline_airflow_mysql_powerbi
    cd pipeline-dados-airflow-mysql-powerbi
    ```

2.  **Ajustar Permiss√µes de Pastas**

O Airflow no Docker precisa de permiss√µes de escrita nas pastas de logs e plugins.
```bash
    sudo mkdir -p ./logs ./plugins
    sudo chown -R 50000:0 ./logs ./plugins
```

3.  **Construir e Iniciar os Cont√™ineres**

Este comando ir√° construir a imagem customizada do Airflow (com `Faker` instalado) e iniciar todos os servi√ßos (Airflow, MySQL, Redis).
```bash
    docker-compose up -d --build
```
Aguarde alguns minutos para que todos os servi√ßos estejam no ar.

4.  **Configurar a Conex√£o no Airflow**
    * Acesse a interface do Airflow em `http://localhost:8080` (login: `admin`, senha: `admin`).
    * V√° em **Admin -> Connections** e crie a conex√£o para o banco de dados da aplica√ß√£o:
        * **Connection Id:** `mysql_default`
        * **Connection Type:** `MySQL`
        * **Host:** `mysql_db`
        * **Schema:** `dados_api`
        * **Login:** `mysql_user`
        * **Password:** `mysql_pass`
        * **Port:** `3306`
    * Teste e salve a conex√£o.
   
5.  **Executar as DAGs**
    * Na interface do Airflow, ative e execute as DAGs na seguinte ordem para construir o Data Mart:
        1. `build_dim_date` (Roda uma vez para criar e popular a tabela de calend√°rio)
        2.  `build_dim_dealers_fictitious` (Roda uma vez para criar as concession√°rias)
        3.  `build_dim_models` (Pode ser rodada periodicamente para atualizar os modelos)
        4.  `build_fact_sales` (Roda por √∫ltimo, pois depende das outras. Pode ser rodada diariamente)

---

## üìä An√°lises no Power BI

Com o Data Mart populado, conecte o Power BI ao banco de dados `dados_api` (Host: `localhost`, Porta: `3306`, Usu√°rio: `mysql_user`) para criar um dashboard interativo e responder perguntas de neg√≥cio como:
* KPIs: Qual o Faturamento Total, Total de Carros Vendidos e o Pre√ßo M√©dio por Venda no per√≠odo selecionado?
* Performance de Produto: Qual o faturamento por marca, e como ele se divide entre os segmentos (Luxo, Esportivo, Geral)?
* Tend√™ncia Temporal: Qual a tend√™ncia de faturamento ao longo dos anos?
* An√°lise Geogr√°fica: Qual a distribui√ß√£o do faturamento total por estado?
* Comparativo de Mercado: Qual a participa√ß√£o de carros a combust√£o vs. el√©tricos no faturamento total?
* Volume de Vendas: Quais marcas vendem mais em volume de unidades?

---

## üë®‚Äçüíª Autor

**Thiago Marchi de Morais**

* [LinkedIn](https://www.linkedin.com/in/thiago-marchi/)
* [GitHub](https://github.com/ThiagoMarchi)
